{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d90153c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.2\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import splitfolders\n",
    "\n",
    "tfk = tf.keras\n",
    "tfkl = tf.keras.layers\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e76d545b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and import visualkeras library\n",
    "\n",
    "import visualkeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c99fce6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random seed for reproducibility\n",
    "seed = 42\n",
    "\n",
    "random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "tf.compat.v1.set_random_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e52e2f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import logging\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=Warning)\n",
    "tf.get_logger().setLevel('INFO')\n",
    "tf.autograph.set_verbosity(0)\n",
    "\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6aafa2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (256, 256, 3)\n",
    "\n",
    "labels = ['Species1',       # 0\n",
    "          'Species2',       # 1\n",
    "          'Species3',       # 2\n",
    "          'Species4',       # 3\n",
    "          'Species2',       # 4\n",
    "          'Species2',       # 5\n",
    "          'Species7',       # 6\n",
    "          'Species8',       # 7\n",
    "          ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3dd80e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Species1', 'Species2', 'Species3', 'Species4', 'Species5', 'Species6', 'Species7', 'Species8']\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "path = \"dataset\"\n",
    "\n",
    "dirs = os.listdir(path)\n",
    "\n",
    "print(dirs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "def resize_aspect_fit():\n",
    "    for d in dirs:\n",
    "        for item in d:\n",
    "            if os.path.isfile(path+ \"/\" +d + \"/\"+item):\n",
    "                image = Image.open(path+ \"/\" +d + \"/\"+item)\n",
    "\n",
    "                new_image_height = 256\n",
    "                new_image_length = 256\n",
    "\n",
    "                image = image.resize((new_image_height, new_image_length), Image.ANTIALIAS)\n",
    "                image.save(path+ \"/\" +d + \"/\"+item, 'JPEG', quality=90)\n",
    "\n",
    "\n",
    "resize_aspect_fit()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4270e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying files: 3542 files [00:06, 561.23 files/s]\n"
     ]
    }
   ],
   "source": [
    "# Splitting the main dataset into train and val\n",
    "dataset_dir = 'datasetNoTest'\n",
    "\n",
    "if not(os.path.exists('../datasetNoTest')) :\n",
    "    print('splitting')\n",
    "    splitfolders.ratio('dataset', output='datasetNoTest', seed=seed, ratio=(0.8, 0.2))\n",
    "\n",
    "# Setting dataset directories\n",
    "training_dir = os.path.join(dataset_dir, 'train')\n",
    "validation_dir = os.path.join(dataset_dir, 'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be0d4a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input Parameters\n",
    "img_w = 96\n",
    "img_h = 96\n",
    "input_shape = (96, 96, 3)\n",
    "classes = 8\n",
    "\n",
    "# Training Parameters\n",
    "epochs = 90\n",
    "batch_size = 64\n",
    "reg_rate = 0.001\n",
    "\n",
    "# Earlystopping Parameters\n",
    "early_stopping = False\n",
    "patience_epochs = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b81f2e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2829 images belonging to 8 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "training_dir = 'datasetNoTest/train'\n",
    "\n",
    "# Create an instance of ImageDataGenerator with Data Augmentation\n",
    "aug_train_data_gen = ImageDataGenerator(rotation_range=30,\n",
    "                                        height_shift_range=50,\n",
    "                                        width_shift_range=50,\n",
    "                                        zoom_range=0.3,\n",
    "                                        horizontal_flip=True,\n",
    "                                        vertical_flip=True, \n",
    "                                        fill_mode='reflect',\n",
    "                                        rescale=1/255.) # rescale value is multiplied to the image\n",
    "\n",
    "# Obtain a data generator with the 'ImageDataGenerator.flow_from_directory' method\n",
    "aug_train_gen = aug_train_data_gen.flow_from_directory(directory=training_dir,\n",
    "                                                       target_size=(96,96),\n",
    "                                                       color_mode='rgb',\n",
    "                                                       classes=None, # can be set to labels\n",
    "                                                       class_mode='categorical',\n",
    "                                                       batch_size=8,\n",
    "                                                       shuffle=True,\n",
    "                                                       seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "313e42d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 713 images belonging to 8 classes.\n"
     ]
    }
   ],
   "source": [
    "valid_data_gen = ImageDataGenerator(rescale=1/255.)\n",
    "validation_dir = 'datasetNoTest/val'\n",
    "valid_gen = valid_data_gen.flow_from_directory(directory=validation_dir,\n",
    "                                               target_size=(96,96),\n",
    "                                               color_mode='rgb',\n",
    "                                               classes=None, # can be set to labels\n",
    "                                               class_mode='categorical',\n",
    "                                               batch_size=8,\n",
    "                                               shuffle=False,\n",
    "                                               seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0dd3f688",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_shape):\n",
    "\n",
    "    # Build the neural network layer by layer\n",
    "    input_layer = tfkl.Input(shape=input_shape, name='input_layer')\n",
    "\n",
    "    conv1 = tfkl.Conv2D(\n",
    "        filters=32,\n",
    "        kernel_size=3,\n",
    "        padding = 'same',\n",
    "        activation = 'relu',\n",
    "        kernel_initializer = tfk.initializers.HeUniform(seed)\n",
    "    )(input_layer)\n",
    "    pool1 = tfkl.MaxPooling2D()(conv1)\n",
    "\n",
    "    conv2 = tfkl.Conv2D(\n",
    "        filters=64,\n",
    "        kernel_size=3,\n",
    "        padding = 'same',\n",
    "        activation = 'relu',\n",
    "        kernel_initializer = tfk.initializers.HeUniform(seed)\n",
    "    )(pool1)\n",
    "    pool2 = tfkl.MaxPooling2D()(conv2)\n",
    "\n",
    "    conv3 = tfkl.Conv2D(\n",
    "        filters=128,\n",
    "        kernel_size=3,\n",
    "        padding = 'same',\n",
    "        activation = 'relu',\n",
    "        kernel_initializer = tfk.initializers.HeUniform(seed)\n",
    "    )(pool2)\n",
    "    pool3 = tfkl.MaxPooling2D()(conv3)\n",
    "\n",
    "    conv4 = tfkl.Conv2D(\n",
    "        filters=256,\n",
    "        kernel_size=3,\n",
    "        padding = 'same',\n",
    "        activation = 'relu',\n",
    "        kernel_initializer = tfk.initializers.HeUniform(seed)\n",
    "    )(pool3)\n",
    "    pool4 = tfkl.MaxPooling2D()(conv4)\n",
    "\n",
    "    conv5 = tfkl.Conv2D(\n",
    "        filters=512,\n",
    "        kernel_size=3,\n",
    "        padding = 'same',\n",
    "        activation = 'relu',\n",
    "        kernel_initializer = tfk.initializers.HeUniform(seed)\n",
    "    )(pool4)\n",
    "    pool5 = tfkl.MaxPooling2D()(conv5)\n",
    "\n",
    "    flattening_layer = tfkl.Flatten(name='Flatten')(pool5)\n",
    "    dropout = tfkl.Dropout(0.3, seed=seed)(flattening_layer)\n",
    "    classifier_layer = tfkl.Dense(units=512, name='Classifier', kernel_initializer=tfk.initializers.HeUniform(seed), activation='relu')(dropout)\n",
    "    dropout = tfkl.Dropout(0.3, seed=seed)(classifier_layer)\n",
    "    output_layer = tfkl.Dense(units=8, activation='softmax', kernel_initializer=tfk.initializers.GlorotUniform(seed), name='output_layer')(dropout)\n",
    "\n",
    "    # Connect input and output through the Model class\n",
    "    model = tfk.Model(inputs=input_layer, outputs=output_layer, name='model')\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.Adam(), metrics='accuracy')\n",
    "\n",
    "    # Return the model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc1cdb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to create folders and callbacks for training\n",
    "from datetime import datetime\n",
    "\n",
    "def create_folders_and_callbacks(model_name):\n",
    "\n",
    "  exps_dir = os.path.join('data_augmentation_experiments')\n",
    "  if not os.path.exists(exps_dir):\n",
    "      os.makedirs(exps_dir)\n",
    "\n",
    "  now = datetime.now().strftime('%b%d_%H-%M-%S')\n",
    "\n",
    "  exp_dir = os.path.join(exps_dir, model_name + '_' + str(now))\n",
    "  if not os.path.exists(exp_dir):\n",
    "      os.makedirs(exp_dir)\n",
    "      \n",
    "  callbacks = []\n",
    "\n",
    "  # Model checkpoint\n",
    "  # ----------------\n",
    "  ckpt_dir = os.path.join(exp_dir, 'ckpts')\n",
    "  if not os.path.exists(ckpt_dir):\n",
    "      os.makedirs(ckpt_dir)\n",
    "\n",
    "  ckpt_callback = tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(ckpt_dir, 'cp.ckpt'), \n",
    "                                                     save_weights_only=True, # True to save only weights\n",
    "                                                     save_best_only=False) # True to save only the best epoch \n",
    "  callbacks.append(ckpt_callback)\n",
    "\n",
    "  # Visualize Learning on Tensorboard\n",
    "  # ---------------------------------\n",
    "  tb_dir = os.path.join(exp_dir, 'tb_logs')\n",
    "  if not os.path.exists(tb_dir):\n",
    "      os.makedirs(tb_dir)\n",
    "      \n",
    "  # By default shows losses and metrics for both training and validation\n",
    "  tb_callback = tf.keras.callbacks.TensorBoard(log_dir=tb_dir, \n",
    "                                               profile_batch=0,\n",
    "                                               histogram_freq=1)  # if > 0 (epochs) shows weights histograms\n",
    "  callbacks.append(tb_callback)\n",
    "\n",
    "  # Early Stopping\n",
    "  # --------------\n",
    "  es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True)\n",
    "  callbacks.append(es_callback)\n",
    "\n",
    "  return callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "641c47d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_layer (InputLayer)    [(None, 96, 96, 3)]       0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 96, 96, 32)        896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 48, 48, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 48, 48, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 24, 24, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 24, 24, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 12, 12, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 12, 12, 256)       295168    \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 6, 6, 256)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 6, 6, 512)         1180160   \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 3, 3, 512)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " Flatten (Flatten)           (None, 4608)              0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 4608)              0         \n",
      "                                                                 \n",
      " Classifier (Dense)          (None, 512)               2359808   \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " output_layer (Dense)        (None, 8)                 4104      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,932,488\n",
      "Trainable params: 3,932,488\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build model (for NO augmentation training)\n",
    "model = build_model(input_shape)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a78302cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/90\n",
      "354/354 [==============================] - 53s 144ms/step - loss: 2.1135 - accuracy: 0.1626 - val_loss: 2.4492 - val_accuracy: 0.1585\n",
      "Epoch 2/90\n",
      "354/354 [==============================] - 55s 155ms/step - loss: 1.8644 - accuracy: 0.2517 - val_loss: 2.0597 - val_accuracy: 0.2454\n",
      "Epoch 3/90\n",
      "354/354 [==============================] - 49s 140ms/step - loss: 1.7097 - accuracy: 0.3400 - val_loss: 1.9659 - val_accuracy: 0.2496\n",
      "Epoch 4/90\n",
      "354/354 [==============================] - 51s 145ms/step - loss: 1.6126 - accuracy: 0.3856 - val_loss: 2.4448 - val_accuracy: 0.2468\n",
      "Epoch 5/90\n",
      "354/354 [==============================] - 52s 146ms/step - loss: 1.5480 - accuracy: 0.4100 - val_loss: 2.1911 - val_accuracy: 0.2440\n",
      "Epoch 6/90\n",
      "354/354 [==============================] - 50s 142ms/step - loss: 1.4828 - accuracy: 0.4316 - val_loss: 1.8629 - val_accuracy: 0.3100\n",
      "Epoch 7/90\n",
      "354/354 [==============================] - 52s 146ms/step - loss: 1.4365 - accuracy: 0.4514 - val_loss: 1.7517 - val_accuracy: 0.3520\n",
      "Epoch 8/90\n",
      "354/354 [==============================] - 48s 136ms/step - loss: 1.3882 - accuracy: 0.4786 - val_loss: 1.8899 - val_accuracy: 0.3394\n",
      "Epoch 9/90\n",
      "354/354 [==============================] - 51s 143ms/step - loss: 1.3461 - accuracy: 0.4906 - val_loss: 1.7614 - val_accuracy: 0.3548\n",
      "Epoch 10/90\n",
      "354/354 [==============================] - 49s 139ms/step - loss: 1.3279 - accuracy: 0.4910 - val_loss: 1.6194 - val_accuracy: 0.3955\n",
      "Epoch 11/90\n",
      "354/354 [==============================] - 51s 144ms/step - loss: 1.3054 - accuracy: 0.5041 - val_loss: 1.8274 - val_accuracy: 0.3114\n",
      "Epoch 12/90\n",
      "354/354 [==============================] - 51s 144ms/step - loss: 1.2482 - accuracy: 0.5352 - val_loss: 1.5223 - val_accuracy: 0.4432\n",
      "Epoch 13/90\n",
      "354/354 [==============================] - 48s 135ms/step - loss: 1.2618 - accuracy: 0.5338 - val_loss: 1.4242 - val_accuracy: 0.4825\n",
      "Epoch 14/90\n",
      "354/354 [==============================] - 54s 151ms/step - loss: 1.2030 - accuracy: 0.5543 - val_loss: 1.3581 - val_accuracy: 0.4881\n",
      "Epoch 15/90\n",
      "354/354 [==============================] - 47s 132ms/step - loss: 1.2301 - accuracy: 0.5433 - val_loss: 1.4629 - val_accuracy: 0.4741\n",
      "Epoch 16/90\n",
      "354/354 [==============================] - 49s 139ms/step - loss: 1.2108 - accuracy: 0.5592 - val_loss: 1.4207 - val_accuracy: 0.4853\n",
      "Epoch 17/90\n",
      "354/354 [==============================] - 54s 153ms/step - loss: 1.1952 - accuracy: 0.5461 - val_loss: 2.0946 - val_accuracy: 0.3282\n",
      "Epoch 18/90\n",
      "354/354 [==============================] - 48s 136ms/step - loss: 1.1916 - accuracy: 0.5613 - val_loss: 1.3982 - val_accuracy: 0.4769\n",
      "Epoch 19/90\n",
      "354/354 [==============================] - 51s 144ms/step - loss: 1.1684 - accuracy: 0.5620 - val_loss: 1.2088 - val_accuracy: 0.5498\n",
      "Epoch 20/90\n",
      "354/354 [==============================] - 53s 148ms/step - loss: 1.1402 - accuracy: 0.5818 - val_loss: 1.3812 - val_accuracy: 0.5344\n",
      "Epoch 21/90\n",
      "354/354 [==============================] - 49s 138ms/step - loss: 1.1308 - accuracy: 0.5875 - val_loss: 1.5693 - val_accuracy: 0.4797\n",
      "Epoch 22/90\n",
      "354/354 [==============================] - 50s 141ms/step - loss: 1.1336 - accuracy: 0.5984 - val_loss: 1.5814 - val_accuracy: 0.4642\n",
      "Epoch 23/90\n",
      "354/354 [==============================] - 53s 149ms/step - loss: 1.1033 - accuracy: 0.6066 - val_loss: 1.3262 - val_accuracy: 0.4937\n",
      "Epoch 24/90\n",
      "354/354 [==============================] - 52s 145ms/step - loss: 1.0850 - accuracy: 0.5981 - val_loss: 1.4930 - val_accuracy: 0.4348\n",
      "Epoch 25/90\n",
      "354/354 [==============================] - 54s 153ms/step - loss: 1.0790 - accuracy: 0.6020 - val_loss: 1.4637 - val_accuracy: 0.4783\n",
      "Epoch 26/90\n",
      "354/354 [==============================] - 57s 162ms/step - loss: 1.0904 - accuracy: 0.6151 - val_loss: 1.2949 - val_accuracy: 0.5175\n",
      "Epoch 27/90\n",
      "354/354 [==============================] - 54s 152ms/step - loss: 1.0697 - accuracy: 0.6133 - val_loss: 1.5341 - val_accuracy: 0.4600\n",
      "Epoch 28/90\n",
      "354/354 [==============================] - 47s 132ms/step - loss: 1.0713 - accuracy: 0.6108 - val_loss: 1.2467 - val_accuracy: 0.5428\n",
      "Epoch 29/90\n",
      "354/354 [==============================] - 47s 133ms/step - loss: 1.0522 - accuracy: 0.6204 - val_loss: 1.4105 - val_accuracy: 0.4797\n"
     ]
    }
   ],
   "source": [
    "# Create folders and callbacks and fit\n",
    "aug_callbacks = create_folders_and_callbacks(model_name='CNN_Aug')\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    x = aug_train_gen,\n",
    "    epochs = epochs,\n",
    "    validation_data = valid_gen,\n",
    "    callbacks = aug_callbacks,\n",
    ").history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8d6a5081",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    }
   ],
   "source": [
    "# Save best epoch model\n",
    "model.save(\"data_augmentation_experiments/CNN_Aug_Best\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
